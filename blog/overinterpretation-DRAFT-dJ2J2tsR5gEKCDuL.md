---
layout: default
---

# An Adversarial Perspective on "Overinterpretation Reveals Image Classification Model Pathologies"

Jacob Springer â€” Jan 9, 2022

## I  Introduction

A group at MIT recently released a paper titled "Overinterpretation reveals image classification model pathologies" (Carter et al., 2021), arguing that image classifiers can often classify images correctly with only a tiny non-human-interpretable subset of the pixels of the original image.

<img title="" src="https://sprin.xyz/assets/blog/overinterpretation/figure1.png" alt="Figure 1" data-align="inline">

Figure 1.  A reproduction of Figure 4 from Carter et al. (2021). This figure shows the original images (top), the sufficient input subset computed by the Batched Input SIS method (middle), and the corresponding masked computed by the Batched Input SIS method (bottom).

The authors present an algorithm, based on an algorithm from a previous paper from the same group (Carter et al., 2019), to leverage the gradient of an image classifier to find a Sufficient Input Subset (SIS). This algorithm, called Batched Gradient SIS (BGSIS), iteratively removes pixels (by setting pixels to gray) that least decrease the confidence. Carter et al., 2021 argue:

> "In contrast to adversarial examples that modify images with extra information, overinterpretation is based on real patterns already present in the training data that also generalize to the test distribution." (p. 1)

In this article, we will argue that the set of pixels that Batched Gradient SIS discovers as "sufficient" are often not sufficient, and rather it is the patterns introduced by the removed (i.e. gray) pixels that add information to the image to maintain or increase the confidence of a particular class. We will argue that the patterns introduced by these removed pixels act like adversarial examples. 

## II  The Batched Gradient SIS method does not find sufficient input subsets

If a candidate input subset is sufficient, then we should be able to reintroduce previously removed pixels in a reasonable (i.e., non-adversarial) way, and the classifier should maintain its prediction. First, we will find a candidate sufficient input subset using the BGSIS method, and then, we will "smooth" this subset by reintroducing pixels that are close to the border of the candidate sufficient input subset. This will effectively blur the border, so that the fine-detail pattern created by the border itself cannot contribute to the confidence of the classifier.

<img title="" src="https://sprin.xyz/assets/blog/overinterpretation/figure2.png" alt="">

Figure 2. A comparison between sufficient input subsets generated by BGSIS and the corresponding smoothed sufficient input subset, generated by re-introducing pixels into the sufficient input subset that border a pixel originally kept by the BGSIS method. This figure shows the original images (top), the BGSIS sufficient input subsets (middle), and the smoothed sufficient input subsets (bottom). For every image, we show the confidence of true label computed by the classifier. 

The smoothed masks are constructed by convolving the mask with a 3x3 matrix of 1's, thus removing from the mask any pixel that is adjacent (including diagonally) to a pixel that was kept by BGSIS. We present the results of this experiment in Figure 2. We see that the confidence of each classifier on the smoothed pixel subsets is near-zero. The substantially smaller confidence of the classifier on the smoothed pixel subsets suggests that the confidence of the classifier on the BGSIS pixel subsets can be, in large part, attributed to the patterns generated from the boundary of the removed pixels themselves. We compute the mean confidence of the classifier on the original images to be 0.72, while the mean confidence on BGSIS pixel subsets is 0.92, and the mean confidence on smoothed pixel subsets is 0.03. Thus, the trend holds across the entire tested dataset. 

## III  Gradient-based saliency of sufficient input subsets is concentrated around the boundary of removed pixels

If a classifier relies on the subset of pixels identified by the Batched Gradient SIS method, then gradient-based saliency methods should identify these pixels as highly salient. We run the SmoothGrad method (Smilkov et al., 2017) to determine pixel saliency of candidate sufficient input subsets. We overlay the saliency maps on top of the corresponding pixel masks generated by BGSIS.

<img title="" src="https://sprin.xyz/assets/blog/overinterpretation/figure3.png" alt="">

Figure 3. Saliency of sufficient input subset images superimposed over the corresponding mask. Yellow pixels correspond to the masked pixels and purple pixels correspond with the pixels remaining in the generated sufficient input subset. White semi-transparent pixels represent the pixels for which the computed saliency of a the pixel was at least one standard deviation greater than the mean saliency.

We observe that the saliency (white semi-transparent pixels) is concentrated around the pixels that border the sufficient input subset, again suggesting that the classification confidence is supported by the patterns generated by the removed pixels themselves, rather than the pixels of the sufficient input subset. This suggests that the bulk of the pixels present in the sufficient input subset generated by BGSIS is unimportant to the model. Instead, the patterns introduced by the borders of the removed pixels have the highest saliency, and thus likely support the classification. This explains the result from the previous section: since the patterns introduced by the border between removed and non-removed pixels are the primary salient pixels, removing them by smoothing the borders will reduce the classification confidence substantially.

## IV  Sufficient input subsets are adversarial examples.

We present an animated visualization of the BGSIS method. We use a pixel batch size of 256 and run the algorithm. We present the pixel mask, the resulting masked image, and the classifier confidence for each iteration of the algorithm. The BGSIS algorithm outputs the image with the most number of masked pixels such that the confidence is above the threshold. This means that the algorithm will select the masked image with the highest iteration number where this masked image is classified with above-threshold confidence and that no future iteration of masked image will be classified with above-threshold confidence. 

<center><video class="video-background" autoplay loop muted playsinline width="480px">
<source src="https://sprin.xyz/assets/blog/overinterpretation/figure4.mp4" type="video/mp4" />
</video></center>

Figure 4. Animated visualization of the BGSIS algorithm and corresponding classification confidence over each iteration in the algorithm.

There are two key takeaways from Figure 4:

1. As the algorithm runs, confidence near-universally increases to nearly 100%. While the algorithm is designed to remove information by removing pixels thus lowering confidence over each iteration, it is instead evident that removing pixels increases the confidence, suggesting that the removed pixels introduce patterns that support classification confidence. As discussed above, we suspect that the patterns created at the boundary of the removed pixels increase the classification confidence.
2. Notably the confidence of the classifier on bottom right image starts at nearly 0%. This is the case when the classifier incorrectly classifies the original image. However, Figure 4 shows that the BGSIS method increases the confidence of this image well above the 90% threshold, suggesting that even when patterns supporting a particular class are not present or only minimally present, removing pixels will increase the confidence.

These results suggest that the BGSIS method is essentially a search for adversarial image masks. The algorithm computes the gradient of the confidence with respect to the mask, and then adjusts the mask to minimally decrease (i.e., maximally increase) the confidence. Broadly, this is the same structure as a gradient-based search for an adversarial example. Thus, it is no surprise that this algorithm will act similarly to any adversarial example search algorithm.

The authors include a similar figure to our Figure 4 in the appendix of their paper (Figure S15 from their paper). Their figure illustrates the classifier confidence for each iteration of the algorithm, and includes substantially more data than our Figure 4. Similarly to our Figure 4, they find that the mean confidence of the classifier increases quickly to nearly 100%.

To explore this idea that this method can be used to construct adversarial masks in more depth, we construct explicitly-adversarial pixel masks using the BGSIS algorithm that aim to cause the classifier to identify a particular (adversarial) target label with a confidence above 90%. Instead of removing pixels to increase the confidence of the true label of the image, we removing pixels to increase the confidence of the adversarial target label.

<img title="" src="https://sprin.xyz/assets/blog/overinterpretation/figure5.png" alt="">

Figure 5. Class-targeted adversarial pixel masks generated using BGSIS. We render pairs of the original image and the confidence on the true label (top) and the adversarially-masked image and the confidence on the adversarial target label (bottom) .

These sufficient input subset adversarial examples have similar properties to the original sufficient input subsets: for many of the examples, the confidence of the classifier is above 90%, and the masks cover most of the images. Consistent with our previous experiments, the existance of adversarial BGSIS masks suggests that the masks themselves carry information pertinent to classification.

To illustrate this idea visually, we present (somewhat cherry picked) masks generated using adversarially-trained classifiers which are known to have more-human-aligned gradients (Engstrom et al., 2019).

<img title="" src="https://sprin.xyz/assets/blog/overinterpretation/figure6.png" alt="">

Figure 6. BGSIS-generated sufficient input subsets and masks on an adversarially-trained ResNet50 (Salman et al., 2020).

The removed pixels form interesting patterns that appear semantic when the classifier is robust (Figure 6). The semantic nature of the mask is not (always) an artifact of outlining a semantic feature present in the original image. For example, the image of the ears of corn are masked to appear as ears of corn that do not align with the original ears of corn. Similarly, the image of the baseball is masked to include the appearence of a larger baseball than appears in the original image. The image of the panda is masked to include the apparence of either whiskers or possibly long thin bamboo-type leaves. While this experiment is less rigorous, it demonstrates visually how removed pixels can form patterns that are meaningful to the classifier.

## V  Removed pixels are real statistical patterns.

The authors find that they can train models on the 5% pixel-subset images that attain minimal accuracy loss compared to the original models and they conclude that the 5% pixel subset has real statistical patterns that correlate with the label well enough on which to base a prediction. We would suspect that a better experiment to determine if this is true would be to train on the smoothed masked images from above, although it is not an experiment that we will present here due to the computational cost of training a new model. As an alternative hypothesis, it is possible that the patterns generated by the removed pixels are real statistical patterns that are useful for classification. The idea that you can train a classifier on adversarial examples and yield non-trivial generalization accuracy has been shown by Ilyas et al. (2019) in the paper "Adversarial Examples Are Not Bugs, They Are Features". A result suggesting that training on the adversarial patterns introduced by the removed pixels of sufficient input subsets generated by BGSIS would be consistent with Ilyas et al.

## VI  Conclusion

The authors of this paper raise a very important point: be very critical of any attempt to interpret a deep learning model, as they are often subject to the bias of the interpretation algorithm.

We don't think that the results that the authors present are wrong or uninteresting---quite the contrary. It is striking to us that 95% or more of the pixels in an image can be replaced with gray and that a classifier will maintain confidence in its classification. However, unless there is a way to explicitly remove input from the model, the interaction between the removed input and the remaining input, such as the patterns made at the border of the removed grey pixels, will always have a possibility of affecting the behavior of a classifier. 

Perhaps a better approach would be to define a sufficient input subset as a subset of pixels such that, if present, will *always* cause the classifier to confidently predict the label under any perturbation to the other pixels. We doubt this will be useful, firstly because we would speculate that such a set would be difficult to compute, and secondly because neural networks are so vulnerable to adversarial examples that such a set would likely consist of nearly every pixel in the original image, so as to not introduce many degrees of freedom that might allow a vulnerability to arise. 

We would speculate that similar approaches to interpretability that aim to determine the contribution of individual pixels will be vulnerable to similar adversarial explanation as is described by this article. Thus, any attempt to interpret a deep learning model should be met with skepticism, as often the bias of the algorithm will shine through.

The code to reproduce our experiments is available at [todo].

## *References*

[todo]

- Carter et al., 2021

- Carter et al., 2019

- Smilkov et al., 2017

- Engstrom et al., 2019

- Salman et al., 2020

- Ilyas et al., 2019

[back](./)
