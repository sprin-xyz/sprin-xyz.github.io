<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jacob Springer | Academic Profile</title>
  <!-- Link to external CSS -->
  <link rel="stylesheet" href="style.css">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700&display=swap" rel="stylesheet">
</head>
<body>
  <div class="container">
    <!-- Profile Section -->
    <header class="profile-header">
      <h1><a href="https://sprin.xyz/">Jacob Springer</a></h1>
      <img src="jacob.png" alt="Jacob Springer" class="profile-picture">
      <div class="profile-links">
        <a href="cv.pdf" target="_blank">CV</a>
        <a href="https://scholar.google.com/citations?user=niZiN38AAAAJ" target="_blank">Google Scholar</a>
        <a href="https://github.com/jakespringer" target="_blank">GitHub</a>
        <span>Email: jspringer@cmu.edu</span>
      </div>
    </header>

    <!-- About Section -->
    <section class="about-section">
      <h2>About</h2>
      <p>
        Hello! I am a PhD student in the Machine Learning Department at Carnegie Mellon University where I am fortunate to be advised by 
        <a href="https://www.cs.cmu.edu/~aditirag/">Aditi Raghunathan</a>. My work is supported by the <a href="https://www.nsfgrfp.org" target="_blank">NSF Graduate Research Fellowship</a>.
      </p>
      <p>
        I'm excited about solving mysteries in machine learning. I'm broadly interested in the science surrounding foundation models, though my current research has a focus around optimization, robustness, and inference-time methods. Most recently, I have been thinking about how to train models that are easily and robustly fine-tuned to perform new tasks by design, and especially how optimization can influence this. I am broadly excited about understanding structure of what is learned by neural networks. In the past, I have also spent a lot of time thinking about (adversarial) robustness in neural networks and how we can take insights from neuroscience to improve upon machine learning. Previously, I was an undergrad at Swarthmore College, and I have spent time at Cold Spring Harbor Laboratory, MIT, and Los Alamos National Laboratory, where I worked with many lovely people. Please reach out if you want to chat about anything (I do love talking about research)!
      </p>
    </section>

    <!-- Publications Section -->
    <section class="publications">
      <h2>Selected Publications (<a href="https://scholar.google.com/citations?user=niZiN38AAAAJ" target="_blank">more</a>)</h2>
      <ol>
        <!-- 2024 -->
        <li>
          <strong>2025</strong> – “<a href="https://arxiv.org/abs/2402.15449" target="_blank">Repetition improves language model embeddings</a>”
          <br>          <strong>Jacob Springer</strong>; Suhas Kotha; Daniel Fried; Graham Neubig; Aditi Raghunathan
          <br>
          <small>International Conference on Learning Representations (2025)</small>
        </li>
        <li>
          <strong>2024</strong> – “<a href="https://arxiv.org/abs/2405.20439" target="_blank">Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning</a>”
          <br>          <strong>Jacob Springer</strong>; Vaishnavh Nagarajan; Aditi Raghunathan
          <br>
          <small>International Conference on Learning Representations (2024)</small>
        </li>
        <!-- 2023 -->
        <li>
          <strong>2024</strong> – “<a href="https://arxiv.org/abs/2309.10105" target="_blank">Understanding catastrophic forgetting in language models via implicit inference</a>”
          <br>          Suhas Kotha; <strong>Jacob Springer</strong>; Aditi Raghunathan
          <br>
          <small>International Conference on Learning Representations (2024)</small>
        </li>
        <!-- 2022 -->
        <li>
          <strong>2022</strong> – “<a href="https://openreview.net/forum?id=BGfLS_8j5eq" target="_blank">If you’ve trained one you’ve trained them all: inter-architecture similarity increases with robustness</a>” <b>(Oral)</b>
          <br>          Haydn Jones; <strong>Jacob Springer</strong>; Garrett Kenyon; Juston Moore
          <br>
          <small>Uncertainty in Artificial Intelligence (2022)</small>
        </li>
        <!-- 2021 -->
        <li>
          <strong>2021</strong> – “<a href="https://sprin.xyz/" target="_blank">It's hard for neural networks to learn the game of life</a>”
          <br>          <strong>Jacob Springer</strong>; Garrett Kenyon
          <br>
          <small>International Joint Conference on Neural Networks (2021)</small>
        </li>
        <li>
          <strong>2021</strong> – “<a href="https://arxiv.org/abs/2106.02105" target="_blank">A little robustness goes a long way: Leveraging robust features for targeted transfer attacks</a>”
          <br>          <strong>Jacob Springer</strong>; Melanie Mitchell; Garrett Kenyon
          <br>
          <small>Advances in Neural Information Processing Systems (2021)</small>
        </li>
        <li>
          <strong>2021</strong> – “<a href="https://arxiv.org/abs/2102.05110" target="_blank">Adversarial perturbations are not so weird: Entanglement of robust and non-robust features in neural network classifiers</a>”
          <br>          <strong>Jacob Springer</strong>; Melanie Mitchell; Garrett Kenyon
          <br>
          <small>Preprint.</small>
        </li>
      </ol>
    </section>
    <footer>
    </footer>
  </div>
</body>
</html>

